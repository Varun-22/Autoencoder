# -*- coding: utf-8 -*-
"""single_cell_denoise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MidDREAt8ahXPFzE8AlAGA_nDN5uYUgx
"""


import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from google.colab import files
from PIL import Image
import random

def uploadfile():
  dataset = []
  uploaded = files.upload()
  labels = list(uploaded.keys())
  for i in labels:
    dataset.append(np.asarray(Image.open(i)))
  dataset = np.asarray(dataset)
  #print(dataset.shape)
  return dataset,labels

def div(trainsize, lab):
  size = len(lab)
  trainnum = (trainsize*size)//100
  tralis = []
  lisx = []
  lisy = []
  teslis = []
  o = 0
  while(o<trainnum):
    j = random.randint(0,len(lab)-1)
    if j not in lisx:
      lisx.append(j)
      tralis.append(lab[j])
      o += 1
  for l in lab:
    if l not in tralis:
      teslis.append(l)
  for i in range(len(lab)):
    if i not in lisx:
      lisy.append(i)
  #print(tralis,teslis)
  return tralis, teslis, lisx, lisy

def matrices(tr,ts,siz):
  trainmat = []
  testmat = []
  for i in tr:
    trainmat.append(dataset[i])
  for i in ts:
    testmat.append(dataset[i])
  trainmat = np.asarray(trainmat)
  testmat = np.asarray(testmat)
  return trainmat, testmat

"""TRAIN DATA"""

dataset, label = uploadfile()

size = len(label)
trainpercent = 100
trainlis, testlis,x,y = div(trainpercent, label)
train, test = matrices(x,y,size)

train = train.astype('float32') / 255
test = test.astype('float32') / 255

train_new = []
for i in range(train.shape[0]):
  train_new.append(train[i].flatten())
test_new = []
for i in range(test.shape[0]):
  test_new.append(test[i].flatten())
train_new = np.asarray(train_new)
test_new = np.asarray(test_new)
#print(train_new.shape)
#print(test_new.shape)

"""TEST DATA"""

dataset, label = uploadfile()

size = len(label)
trainpercent = 100
trainlis, testlis,x,y = div(trainpercent, label)
train, test = matrices(x,y,size)
print(np.amax(train))

train = train.astype('float32') / 255
test = test.astype('float32') / 255

train_new1 = []
for i in range(train.shape[0]):
  train_new1.append(train[i].flatten())
test_new = []
for i in range(test.shape[0]):
  test_new.append(test[i].flatten())
train_new1 = np.asarray(train_new1)
test_new = train_new1
#print(train_new.shape)
#print(test_new.shape)
print(np.amax(train_new[0]))

"""AUTOENCODER"""

inputsz = 4356
hidden1 = 256
hidden2 = 128
hidden3 = 64
hidden4 = 128
hidden5 = 256

batchsize = 10
lr = 0.01

x = tf.placeholder("float",shape=[None,inputsz])
target_ = tf.placeholder("float",shape=[None,inputsz])

#initialize weights and biases
w = {
    "w1" : tf.Variable(tf.random_normal([inputsz,hidden1])),
    "w2" : tf.Variable(tf.random_normal([hidden1, hidden2])),
    "w3" : tf.Variable(tf.random_normal([hidden2,hidden3])),
    "w4" : tf.Variable(tf.random_normal([hidden3,hidden4])),
    "w5" : tf.Variable(tf.random_normal([hidden4,hidden5])),
    "w6" : tf.Variable(tf.random_normal([hidden5, inputsz]))
}

b = {
    "b1" : tf.Variable(tf.random_normal([hidden1])),
    "b2" : tf.Variable(tf.random_normal([hidden2])),
    "b3" : tf.Variable(tf.random_normal([hidden3])),
    "b4" : tf.Variable(tf.random_normal([hidden4])),
    "b5" : tf.Variable(tf.random_normal([hidden5])),
    "b6" : tf.Variable(tf.random_normal([inputsz]))
}

def encoder(x_):
  a1 = tf.matmul(x_,w["w1"])
  a1 = tf.add(a1,b["b1"])
  a1 = tf.nn.sigmoid(a1)
  print(a1.shape)
  a2 = tf.matmul(a1,w["w2"])
  a2 = tf.add(a2,b["b2"])
  a2 = tf.nn.sigmoid(a2)
  print(a2.shape)
  a3 = tf.matmul(a2,w["w3"])
  a3 = tf.add(a3,b["b3"])
  a3 = tf.nn.sigmoid(a3)
  print(a3.shape)
  return a3

def decoder(a3_):
  print(w["w4"].shape)
  a4 = tf.matmul(a3_,w["w4"])
  a4 = tf.add(a4,b["b4"])
  a4 = tf.nn.sigmoid(a4)
  
  a5 = tf.matmul(a4,w["w5"])
  a5 = tf.add(a5,b["b5"])
  a5 = tf.nn.sigmoid(a5)
  #----------------
  drop_out = tf.nn.dropout(a5,rate = 0.1)
  a6 = tf.matmul(drop_out,w["w6"])
  #------------
  
  
  #a6 = tf.matmul(a5,w["w6"])
  a6 = tf.add(a6,b["b6"])
  a6 = tf.nn.sigmoid(a6)
  
  return a6

def noisy(noise_typ,image):
   if noise_typ == "gauss":
      row,col,ch= image.shape
      mean = 0
      var = 0.1
      sigma = var**0.5
      gauss = np.random.normal(mean,sigma,(row,col,ch))
      gauss = gauss.reshape(row,col,ch)
      noisy = image + gauss
      return noisy

encoded = encoder(x)
decoded = decoder(encoded)

finaloutput = decoded
#actual = x
target = target_
cost = tf.reduce_mean(tf.pow(target-finaloutput,2))
optimize = tf.train.AdamOptimizer(lr)
training = optimize.minimize(cost)

init = tf.global_variables_initializer()
with tf.Session() as sess:
  sess.run(init)
  for i in range(2000):
#    for k in range(train_new.shape[0]//batchsize):
      k=0
      traindata = train_new[k*batchsize:(k+1)*batchsize]
      noisy_img = traindata + 0.1 * np.random.randn(*traindata.shape)
      noisy_img = np.clip(noisy_img, 0., 1.)
      traintrial = traindata[0]
      traintrial = traintrial.reshape([1,4356])
      sess.run(training, feed_dict={x:noisy_img,target_:traintrial})
      
      if i % 10 == 0:
        loss, _ = sess.run([cost,training], feed_dict= {x:noisy_img,target_:traindata})
        print("iter "+str(i)+" Minibatch loss = "+ "{:.4f}".format(loss))
  print("opt fin")
  original = np.empty((66*10,66*10))
  reconstructed = np.empty((66*10,66*10))
  #for t in range(test_new.shape[0]//batchsize):
  for t in range(1):
    testdata = test_new[t*batchsize:(t+1)*batchsize]
    noisy_imgs = testdata + 0.1 * np.random.randn(*testdata.shape)
    noisy_imgs = np.clip(noisy_imgs, 0., 1.)    
    outsinglesample = sess.run(decoded,feed_dict = {x:noisy_imgs})
    out1 = outsinglesample[0].reshape([66,66])
  noisy_imgs = noisy_imgs[0].reshape([66,66])
  in1 = traintrial[0].reshape([66,66])
  plt.figure(figsize=(1,1))
  plt.imshow(noisy_imgs, origin="upper",cmap="gray")
  plt.show()
  
  plt.figure(figsize=(1,1))
  plt.imshow(out1, origin="upper",cmap="gray")
  plt.show()
  #out1 = out1.reshape([66,66,1])
  #noisy_img = noisy_img.reshape([66,66,1])
  #in1 = in1.reshape([66,66,1])
  #a = tf.image.psnr(noisy_img,in1, max_val = 1.0)
  #b = tf.image.psnr(out1,in1, max_val = 1.0)
  #print(sess.run(a))
  #print(sess.run(b))

import cv2
from google.colab.patches import cv2_imshow
a = noisy_imgs
b = out1
c = testdata[0].reshape([66,66])

print(np.amax(a))
print(np.amax(b))
print(np.amax(c))

data = a

#Rescale to 0-255 and convert to uint8
rescaled = (255.0 / data.max() * (data - data.min())).astype(np.uint8)
print(np.amax(rescaled))
im = Image.fromarray(rescaled)
im.save('noisy1.png')
cv2_imshow(rescaled)
cv2.imwrite("n.png",rescaled)
data = b

#Rescale to 0-255 and convert to uint8
rescaled = (255.0 / data.max() * (data - data.min())).astype(np.uint8)
print(np.amax(rescaled))
im = Image.fromarray(rescaled)
im.save('output1.png')
cv2_imshow(rescaled)
cv2.imwrite("o.png",rescaled)
data = c

#Rescale to 0-255 and convert to uint8
rescaled = (255.0 / data.max() * (data - data.min())).astype(np.uint8)
print(np.amax(rescaled))
im = Image.fromarray(rescaled)
im.save('input1.png')
cv2_imshow(rescaled)
cv2.imwrite("i.png",rescaled)